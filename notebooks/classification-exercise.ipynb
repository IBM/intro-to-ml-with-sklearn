{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification data using scikit-learn\n",
    "\n",
    "\n",
    "Classification problems are those in which the feature to be predicted contains categories of values. Each of these categories are considered as a class into which the predicted value will fall into and hence has its name, classification.\n",
    "\n",
    "In this notebook, we'll use scikit-learn to predict classes. scikit-learn provides implementations of many classification algorithms. In here, we have done a comparative study of 3 different classification algorithms. \n",
    "\n",
    "To help visualize what we are doing, we'll use 2D and 3D charts to show how the classes look (with 3 selected dimensions) with matplotlib and scikit-plot python libraries.\n",
    "\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [Load libraries](#load_libraries)\n",
    "2. [Data exploration](#explore_data)\n",
    "3. [Prepare data for building classification model](#prepare_data)\n",
    "4. [Split data into train and test sets](#split_data)\n",
    "5. [Helper methods for graph generation](#helper_methods)\n",
    "6. [Build Naive Bayes classification model](#model_nb)\n",
    "7. [Build Logistic Regression classification model](#model_lrc)\n",
    "8. [Comparative study of different classification algorithms](#compare_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick set of instructions to work through the notebook\n",
    "\n",
    "If you are new to Notebooks, here's a quick overview of how to work in this environment.\n",
    "\n",
    "1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n",
    "2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n",
    "3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n",
    "4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_libraries\"></a>\n",
    "## 1. Load libraries\n",
    "[Top](#top)\n",
    "\n",
    "Install python modules\n",
    "<br>",
    "**NOTE!** Some pip installs require a kernel restart.\n",
    "<br>",
    "The shell command pip install is used to install Python modules. Some installs require a kernel restart to complete. To avoid confusing errors, run the following cell once and then use the Kernel menu to restart the kernel before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==0.24.2\n",
    "!pip install --user pandas_ml==0.6.1\n",
    "#downgrade matplotlib to bypass issue with confusion matrix being chopped out\n",
    "!pip install matplotlib==3.1.0\n",
    "!pip install --user scikit-learn==0.21.3\n",
    "!pip install -q scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import scikitplot as skplt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_data\"></a>\n",
    "## 2. Data exploration\n",
    "[Top](#top)\n",
    "\n",
    "Data can be easily loaded within IBM Watson Studio. Instructions to load data within IBM Watson Studio can be found [here](https://ibmdev1.rtp.raleigh.ibm.com/tutorials/watson-studio-using-jupyter-notebook/). The data set can be located by its name and inserted into the notebook as a pandas DataFrame as shown below.\n",
    "\n",
    "![insert_spark_dataframe.png](https://raw.githubusercontent.com/IBM/icp4d-customer-churn-classifier/master/doc/source/images/insert_spark_dataframe.png)\n",
    "\n",
    "The generated code comes up with a generic name and it is good practice to rename the dataframe to match the use case context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn_pd = pd.read_csv(\"https://raw.githubusercontent.com/IBM/ml-learning-path-assets/master/data/mergedcustomers_missing_values_GENDER.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"Each category within the churnrisk column has the following count : \")\n",
    "print(df_churn_pd.groupby(['CHURNRISK']).size())\n",
    "#bar chart to show split of data\n",
    "index = ['High','Medium','Low']\n",
    "churn_plot = df_churn_pd['CHURNRISK'].value_counts(sort=True, ascending=False).plot(kind='bar',figsize=(4,4),title=\"Total number for occurences of churn risk \" + str(df_churn_pd['CHURNRISK'].count()), color=['#BB6B5A','#8CCB9B','#E5E88B'])\n",
    "churn_plot.set_xlabel(\"Churn Risk\")\n",
    "churn_plot.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare_data\"></a>\n",
    "## 3. Prepare data for building classification model\n",
    "[Top](#top)\n",
    "\n",
    "Data preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes the bulk of a data scientist's time spent building models.\n",
    "\n",
    "During this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices are encoded using One-hot encoding to a binary vector with at most a single-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n",
    "\n",
    "Final step in the data preparation process is to assemble all the categorical and non-categorical columns into a feature vector. We use VectorAssembler for this. VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#remove columns that are not required\n",
    "df_churn_pd = df_churn_pd.drop(['ID'], axis=1)\n",
    "\n",
    "df_churn_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical columns \n",
    "categoricalColumns = ['GENDER', 'STATUS', 'HOMEOWNER']\n",
    "\n",
    "print(\"Categorical columns : \" )\n",
    "print(categoricalColumns)\n",
    "\n",
    "impute_categorical = SimpleImputer(strategy=\"most_frequent\")\n",
    "onehot_categorical =  OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('impute',impute_categorical),('onehot',onehot_categorical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the numerical columns \n",
    "numericalColumns = df_churn_pd.select_dtypes(include=[np.float,np.int]).columns\n",
    "\n",
    "print(\"Numerical columns : \" )\n",
    "print(numericalColumns)\n",
    "\n",
    "scaler_numerical = StandardScaler()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[('scale',scaler_numerical)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns)],\n",
    "                                            remainder=\"passthrough\")\n",
    "preprocessorForAllColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns),('num',numerical_transformer,numericalColumns)],\n",
    "                                            remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "#. The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like\n",
    "df_churn_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_churn_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_churn_pd_temp)\n",
    "\n",
    "df_churn_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_churn_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_churn_pd_temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data frame for splitting data into train and test datasets\n",
    "\n",
    "features = []\n",
    "features = df_churn_pd.drop(['CHURNRISK'], axis=1)\n",
    "\n",
    "label_churn = pd.DataFrame(df_churn_pd, columns = ['CHURNRISK']) \n",
    "label_encoder = LabelEncoder()\n",
    "label = df_churn_pd['CHURNRISK']\n",
    "\n",
    "label = label_encoder.fit_transform(label)\n",
    "print(\"Encoded value of Churnrisk after applying label encoder : \" + str(label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 75\n",
    "x = df_churn_pd['ESTINCOME']\n",
    "y = df_churn_pd['DAYSSINCELASTTRADE']\n",
    "z = df_churn_pd['TOTALDOLLARVALUETRADED']\n",
    "\n",
    "pop_a = mpatches.Patch(color='#BB6B5A', label='High')\n",
    "pop_b = mpatches.Patch(color='#E5E88B', label='Medium')\n",
    "pop_c = mpatches.Patch(color='#8CCB9B', label='Low')\n",
    "def colormap(risk_list):\n",
    "    cols=[]\n",
    "    for l in risk_list:\n",
    "        if l==0:\n",
    "            cols.append('#BB6B5A')\n",
    "        elif l==2:\n",
    "            cols.append('#E5E88B')\n",
    "        elif l==1:\n",
    "            cols.append('#8CCB9B')\n",
    "    return cols\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('2D and 3D view of churnrisk data')\n",
    "\n",
    "# First subplot\n",
    "ax = fig.add_subplot(1, 2,1)\n",
    "\n",
    "ax.scatter(x, y, alpha=0.8, c=colormap(label), s= area)\n",
    "ax.set_ylabel('DAYS SINCE LAST TRADE')\n",
    "ax.set_xlabel('ESTIMATED INCOME')\n",
    "\n",
    "plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "# Second subplot\n",
    "ax = fig.add_subplot(1,2,2, projection='3d')\n",
    "\n",
    "ax.scatter(z, x, y, c=colormap(label), marker='o')\n",
    "\n",
    "ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "ax.set_ylabel('ESTIMATED INCOME')\n",
    "ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "\n",
    "plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"split_data\"></a>\n",
    "## 4. Split data into train and test sets\n",
    "[Top](#top)\n",
    "\n",
    "This will split our sample data into a training set and a testing set. The training set will be the data used to build the model. The testing set will be used as a basis to compare how well the model predicts values.\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,label , random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n",
    "      \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n",
    "      \" Output label\" + str(y_test.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"helper_methods\"></a>\n",
    "## 5. Helper methods for graph generation\n",
    "[Top](#top)\n",
    "\n",
    "These functions will be used to generate the histograms we will use to compare our algorithms against the actual data.\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colormap(risk_list):\n",
    "    cols=[]\n",
    "    for l in risk_list:\n",
    "        if l==0:\n",
    "            cols.append('#BB6B5A')\n",
    "        elif l==2:\n",
    "            cols.append('#E5E88B')\n",
    "        elif l==1:\n",
    "            cols.append('#8CCB9B')\n",
    "    return cols\n",
    "\n",
    "def two_d_compare(y_test,y_pred,model_name):\n",
    "    #y_pred = label_encoder.fit_transform(y_pred)\n",
    "    #y_test = label_encoder.fit_transform(y_test)\n",
    "    area = (12 * np.random.rand(40))**2 \n",
    "    plt.subplots(ncols=2, figsize=(10,4))\n",
    "    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'], alpha=0.8, c=colormap(y_test))\n",
    "    plt.title('Actual')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'],alpha=0.8, c=colormap(y_pred))\n",
    "    plt.title('Predicted')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "x = X_test['TOTALDOLLARVALUETRADED']\n",
    "y = X_test['ESTINCOME']\n",
    "z = X_test['DAYSSINCELASTTRADE']\n",
    "\n",
    "pop_a = mpatches.Patch(color='#BB6B5A', label='High')\n",
    "pop_b = mpatches.Patch(color='#E5E88B', label='Medium')\n",
    "pop_c = mpatches.Patch(color='#8CCB9B', label='Low')\n",
    "\n",
    "def three_d_compare(y_test,y_pred,model_name):\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    fig.suptitle('Actual vs Predicted (3D) data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colormap(y_test), marker='o')\n",
    "    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "    ax.set_ylabel('ESTIMATED INCOME')\n",
    "    ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "    plt.title('Actual')\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colormap(y_pred), marker='o')\n",
    "    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n",
    "    ax.set_ylabel('ESTIMATED INCOME')\n",
    "    ax.set_zlabel('DAYS SINCE LAST TRADE')\n",
    "    plt.legend(handles=[pop_a,pop_b,pop_c])\n",
    "    plt.title('Predicted')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def model_metrics(y_test,y_pred):\n",
    "    print(\"Decoded values of Churnrisk after applying inverse of label encoder : \" + str(np.unique(y_pred)))\n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix(y_test,y_pred,text_fontsize=\"small\",cmap='Greens',figsize=(6,4))\n",
    "    plt.show()\n",
    "    \n",
    "    #print(\"The classification report for the model : \\n\\n\"+ classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_nb\"></a>\n",
    "## 6. Build Naive Bayes classification model\n",
    "[Top](#top)\n",
    "\n",
    "Naive Bayes applies Bayes’ theorem to calculate the probability of a data point belonging to a particular class. Given the probability of certain related values, the formula to calculate the probability of an event B, given event A to occur is calculated as follows.\n",
    "\n",
    "P(B|A) = (P(A|B) * P(B) / P(A))\n",
    "\n",
    "This theory is considered naive, because it assumes that there is no dependency between any of the input features. Even with this not true or naive assumption, the Naive Bayes algorithm has been proven to perform really well in certain use cases like spam filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_name = 'Naive Bayes Classifier'\n",
    "\n",
    "nbClassifier = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "nb_model = Pipeline(steps=[('preprocessor', preprocessorForCategoricalColumns),('classifier', nbClassifier)]) \n",
    "\n",
    "#fit the model to the training data\n",
    "nb_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_nb= nb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(y_test, return_counts=True)\n",
    "frequency_actual = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_nb, return_counts=True)\n",
    "frequency_predicted_nb = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "n_groups = 3\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, frequency_actual, bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Actual')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, frequency_predicted_nb, bar_width,\n",
    "alpha=opacity,\n",
    "color='pink',\n",
    "label='Naive Bayesian - Predicted')\n",
    "\n",
    "\n",
    "plt.xlabel('Churn Risk')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Actual vs Predicted frequency.')\n",
    "plt.xticks(index + bar_width, ('High', 'Medium', 'Low'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_lrc\"></a>\n",
    "## 7. Build Logistic Regression classification model\n",
    "[Top](#top)\n",
    "\n",
    "Logistic regression is an extension to the linear regression algorithm, the details of which were discussed in the previous notebook. In a logistic regression algorithm, instead of predicting the actual continuous value, we predict the probability of a data point belonging to a particular class. To achieve this, a logistic function is applied to the outcome of the linear regression  This outputs a value between 0 and 1. Then, we select a line. Any data point with a probability value above the line is classified into the class represented by 1. Any data point below the line is classified into the class represented by 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Complete the code and set up the pipeline, fit the model, and predict y values. <br />Hint: Look at the structure of the code used for the Naive Bayes model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_name = \"Logistic Regression Classifier\"\n",
    "\n",
    "logisticRegressionClassifier = LogisticRegression(random_state=0,multi_class='auto',solver='lbfgs',max_iter=1000)\n",
    "\n",
    "##Complete the code below \n",
    "lrc_model = \n",
    "\n",
    "#fit the model to the training data\n",
    "\n",
    "y_pred_lrc = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the following cell to see the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load https://raw.githubusercontent.com/IBM/intro-to-ml-with-sklearn/main/notebook-answers/classification-answers-7.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(y_test, return_counts=True)\n",
    "frequency_actual = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_lrc, return_counts=True)\n",
    "frequency_predicted_lrc = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "n_groups = 3\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, frequency_actual, bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Actual')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width*2, frequency_predicted_lrc, bar_width,\n",
    "alpha=opacity,\n",
    "color='y',\n",
    "label='Logistic Regression - Predicted')\n",
    "\n",
    "plt.xlabel('Churn Risk')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Actual vs Predicted frequency.')\n",
    "plt.xticks(index + bar_width, ('High', 'Medium', 'Low'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"compare_classification\"></a>\n",
    "## 8. Comparative study of different classification algorithms \n",
    "[Top](#top)\n",
    "\n",
    "In the bar chart below, we have compared the different classification algorithm against the actual values. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(y_test, return_counts=True)\n",
    "frequency_actual = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_lrc, return_counts=True)\n",
    "frequency_predicted_lrc = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "uniqueValues, occurCount = np.unique(y_pred_nb, return_counts=True)\n",
    "frequency_predicted_nb = (occurCount[0],occurCount[2],occurCount[1])\n",
    "\n",
    "n_groups = 3\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.1\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, frequency_actual, bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Actual')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, frequency_predicted_nb, bar_width,\n",
    "alpha=opacity,\n",
    "color='pink',\n",
    "label='Naive Bayesian - Predicted')\n",
    "\n",
    "rects3 = plt.bar(index + bar_width*2, frequency_predicted_lrc, bar_width,\n",
    "alpha=opacity,\n",
    "color='y',\n",
    "label='Logistic Regression - Predicted')\n",
    "\n",
    "plt.xlabel('Churn Risk')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Actual vs Predicted frequency.')\n",
    "plt.xticks(index + bar_width, ('High', 'Medium', 'Low'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we see that the predictions around the Medium values is low on accuracy. One reason for that could be that the number of entries for the Medium values were much less represented than High and Low values. Further testing can be done by either increasing the number of Medium entries or involving several data fabrication techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=-1 color=gray>\n",
    "&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n",
    "<p>\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\n",
    "except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the\n",
    "License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "express or implied. See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "</font></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
