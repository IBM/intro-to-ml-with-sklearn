{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression data using scikit-learn\n",
    "\n",
    "Regression refers to the process of predicting a dependent variable by analyzing the relationship between other independent variables. There are several common algorithms that help us in excavating these relationships to better predict the value.\n",
    "\n",
    "In this notebook, we'll use `scikit-learn` to predict values. `Scikit-learn` provides implementations of many regression algorithms. In here, we have done a comparative study of 3 different regression algorithms. \n",
    "\n",
    "To help visualize what we are doing, we'll use 2D and 3D charts to show how the classes looks (with 3 selected dimensions) with matplotlib and seaborn python libraries.\n",
    "\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [Load libraries](#load_libraries)\n",
    "2. [Helper methods for metrics](#helper_methods)\n",
    "3. [Data exploration](#explore_data)\n",
    "4. [Prepare data for building regression model](#prepare_data)\n",
    "5. [Build Simple Linear Regression model](#model_slr)\n",
    "6. [Build Multiple Linear Regression model](#model_mlr)\n",
    "7. [Build Polynomial Linear Regression model](#model_plr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick set of instructions to work through the notebook\n",
    "\n",
    "If you are new to Notebooks, here's a quick overview of how to work in this environment.\n",
    "\n",
    "1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n",
    "2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n",
    "3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n",
    "4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_libraries\"></a>\n",
    "## 1. Load libraries\n",
    "[Top](#top)\n",
    "\n",
    " It is convention to import all of your python libraries at the top of the file. While it is possible to import the libraries at any point in a python notebook, doing so all in one place makes it easy to figure out where the symbols are coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Handle categorical columns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Chain a sequence of transformations\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separate the data into Training and Testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Compute performance metrics for models\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error, r2_score\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"helper_methods\"></a>\n",
    "## 2. Helper methods for metrics\n",
    "[Top](#top)\n",
    "\n",
    "One of the benefits of using Python for data science is that you can simplify your work by defining repetitive tasks as functions (or methods as they are called in Python).\n",
    "\n",
    "In the following section, we define three methods that will help us with the repetitive tasks throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def two_d_compare(X_test,y_test,y_pred,model_name):\n",
    "    '''\n",
    "    Plot the predicted values and actual values on two side-by-side plots.\n",
    "\n",
    "    :param X_test: A series containing the X values.\n",
    "    :param y_test: A series containing the actual Y values corresponding to X_test entries.\n",
    "    :param y_pred: A series containing the predicted Y values corresponding to X_test entries.\n",
    "    :param model_name: name of the model. Used for placing in the plot's title.\n",
    "    '''\n",
    "\n",
    "    # Defining a plot with two subplots\n",
    "    plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "    # Naming the plots\n",
    "    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "    # Populating the first subplot\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_test, y_test, alpha=0.8, color='#8CCB9B')\n",
    "    plt.title('Actual')\n",
    "\n",
    "    # Populating the second subplot\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_test, y_pred,alpha=0.8, color='#E5E88B')\n",
    "    plt.title('Predicted')\n",
    "\n",
    "    # directive to display the created plot\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def model_metrics(y_test,y_pred):\n",
    "    '''\n",
    "    Calculate MSE and R2 errors, print them, and return them as a list.append\n",
    "\n",
    "    :param y_test: A series containing the actual Y values\n",
    "    :param y_pred: A series containing the predicted Y values\n",
    "    '''\n",
    "\n",
    "    # Calculate and print Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    print(\"Mean squared error: %.2f\" % mse)\n",
    "    \n",
    "    # Calculate and print R^2 \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print('R2 score: %.2f' % r2 )\n",
    "    \n",
    "    return [mse, r2]\n",
    "\n",
    "def two_vs_three(x_test,y_test,y_pred,z=None, isLinear = False) : \n",
    "    '''\n",
    "    Create a 3D plot of LOT AREA vs YEAR BUILT vs SELLING PRICE.\n",
    "\n",
    "    Technically this function creates 2-D and 3-D scatterplots of the inputs.Since in this \n",
    "    notebook we've only used it to generate the three plot mentioned above, we are hardcoding\n",
    "    the axis names to avoid having to pass them in as parameters every time.\n",
    "\n",
    "    :param x_test: A series containing the x values\n",
    "    :param y_test: A series containing the actual Y values\n",
    "    :param y_pred: A series containing the predicted Y values\n",
    "    '''\n",
    "    \n",
    "    area = 60\n",
    "    \n",
    "    # Define the size of the graph and it's title\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    fig.suptitle('2D and 3D view of sales price data')\n",
    "\n",
    "    # First subplot\n",
    "    ax = fig.add_subplot(1, 2,1)\n",
    "    ax.scatter(x_test, y_test, alpha=0.5,color='blue', s= area)\n",
    "    # ax.plot(x_test, y_pred, alpha=0.9,color='red', linewidth=2)\n",
    "    ax.plot(x_test, y_pred, alpha=0.5,color='red', marker='s', linewidth=0)\n",
    "    ax.set_xlabel('YEAR BUILT')\n",
    "    ax.set_ylabel('SELLING PRICE')\n",
    "    \n",
    "    plt.title('YEARBUILT vs SALEPRICE')\n",
    "    \n",
    "    if not isLinear : \n",
    "    # Second subplot\n",
    "        ax = fig.add_subplot(1,2,2, projection='3d')\n",
    "\n",
    "        ax.scatter(z, x_test, y_test, color='blue', marker='o')\n",
    "        # ax.plot(z, x_test, y_pred, alpha=0.9,color='red', linewidth=2)\n",
    "        ax.plot(z, x_test, y_pred, alpha=0.5,color='red', marker='s', linewidth=0)\n",
    "        ax.set_ylabel('YEAR BUILT')\n",
    "        ax.set_zlabel('SELLING PRICE')\n",
    "        ax.set_xlabel('LOT AREA')\n",
    "\n",
    "    plt.title('LOT AREA vs YEAR BUILT vs SELLING PRICE')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore_data\"></a>\n",
    "## 3. Data exploration\n",
    "[Top](#top)\n",
    "\n",
    "Data can be easily loaded within IBM Watson Studio. Instructions to load data within IBM Watson Studio can be found [here](https://developer.ibm.com/tutorials/watson-studio-using-jupyter-notebook/). The data set can be located by its name and inserted into the notebook as a pandas DataFrame as shown below.\n",
    "\n",
    "![insert_spark_dataframe.png](https://raw.githubusercontent.com/IBM/icp4d-customer-churn-classifier/master/doc/source/images/insert_spark_dataframe.png)\n",
    "\n",
    "The generated code comes up with a generic name and it is good practice to rename the dataframe to match the use case context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify this notebook, we will use a feature of Pandas that allows us to directly load a csv file from the internet. You can use the instruction above if you want to load your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "df_pd =  pd.read_csv(\"https://raw.githubusercontent.com/IBM/ml-learning-path-assets/master/data/predict_home_value.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### About the Data\n",
    "The data that we are loading contains housing related information. With several independent variables related to this domain, we are going to predict the sales price of a house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 rows of the data.\n",
    "# Good for quick inspection of the data and column names.\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating a scatter plot for the price of the house vs. the year the house was built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_column = df_pd['YEARBUILT']\n",
    "price_column = df_pd['SALEPRICE']\n",
    "\n",
    "sns.scatterplot(x = year_column, y =price_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Let's practice our plotting. Create a scatter plot of the lot area versus the sales price. Do you see any trends? Did you notice the outliers?\n",
    "\n",
    "**Hint:** Print the column names first if you don't know the column name. \n",
    "**Note** that you can create a scatter plot only for the numerical columns (`int64` in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer:\n",
    "\n",
    "# Uncomment the line below if you need the column names\n",
    "# df_pd.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution \n",
    "\n",
    "area_column = df_pd['LOTAREA']\n",
    "price_column = df_pd['SALEPRICE']\n",
    "\n",
    "sns.scatterplot(x = area_column, y =price_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, let's take a look at creating histograms using `seaborn`. Note that the `histplot` functionality of Seaborn has many options, in this case we are enabling the KDE by adding `kde = True` to add a kernel density estimate to smooth the distribution.\n",
    "\n",
    "\n",
    " See [histplot documentation](https://seaborn.pydata.org/generated/seaborn.histplot.html) to learn about the other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_pd['SALEPRICE'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learnt how to explore the data visually, let's see how we can find out about the data types in the columns. This gives us the idea of which are numerical and which are categorical so we can apply the correct visualization tool to them.\n",
    "\n",
    "`int64` denotes a 64-bit integer, that is, a numerical value. `object` on the other hand denotes a non-numerical value which in this case we know is a string (text) that indicates categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains columns of the following data types : \\n\" +str(df_pd.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "Notice below that FIREPLACEQU, GARAGETYPE, GARAGEFINISH, GARAGECOND,FENCE and POOLQC have missing values. \n",
    "\n",
    "**Important:** It is important to take care of missing data before feeding the data into your ML model. Most Regression Algorithms cannot handle missing values, so it is on you to decide what to do with them before passing the data to the next step.\n",
    "\n",
    "You could, for instance, remove those rows, fill them in with the average of the column, interpolate based on other rows, or any other statistical method. Nonetheless, you should handle missing values (`NaN` or Not a Number) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Place True for each cell if the value is missing and False if a value is present\n",
    "missing_values = df_pd.isna()\n",
    "\n",
    "# Since True is a 1 and False is a 0, by summing each column, we effectively count the number of Trues \n",
    "# which is equal to the count of missing values.\n",
    "missing_values_count = missing_values.sum()\n",
    "\n",
    "print(\"The dataset contains following number of missing values for each of the columns : \\n\" + str(missing_values_count) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the following code to simply indicate if there are *any* missing values in each column. By the time you start your machine learning experiment, you want to have Falses for every column that is used in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show if there are any missing values in each column\n",
    "df_pd.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare_data\"></a>\n",
    "## 4. Prepare data for building regression model\n",
    "[Top](#top)\n",
    "\n",
    "Data preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes bulk of data scientist's time spent building models.\n",
    "\n",
    "During this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices are encoded using One-hot encoding to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by identifying columns that will not add any value toward predicting the outputs. While some of these columns are easily identified, a subject matter expert is usually engaged to identify most of them. Removing such columns helps in reducing dimensionality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove columns that are not required\n",
    "df_pd = df_pd.drop(['ID'], axis=1)\n",
    "\n",
    "df_pd.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing techniques that are applied must be customized for each of the columns. Sklearn provides a library called the ColumnTransformer, which allows a sequence of these techniques to be applied to selective columns using a pipeline.\n",
    "\n",
    "\n",
    "A common problem while dealing with data sets is that values will be missing. scikit-learn provides a method to fill these empty values with something that would be applicable in its context. We used the SimpleImputer class that is provided by Sklearn and filled the missing values with the most frequent value in the column.\n",
    "\n",
    "\n",
    "Also, because machine learning algorithms perform better with numbers than with strings, we want to identify columns that have categories and convert them into numbers. We use the OneHotEncoder class provided by Sklearn. The idea of one hot encoder is to create binary variables that each represent a category. By doing this, we remove any ordinal relationship that might occur by just assigning numbers to categories. Basically, we go from a single column that contains multiple class numbers to multiple columns that contain only binary class numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical columns \n",
    "categoricalColumns = df_pd.select_dtypes(include=[object]).columns\n",
    "\n",
    "print(\"Categorical columns : \" )\n",
    "print(categoricalColumns)\n",
    "\n",
    "impute_categorical = SimpleImputer(strategy=\"most_frequent\")\n",
    "onehot_categorical =  OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('impute',impute_categorical),('onehot',onehot_categorical)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical columns from the data set are identified, and StandardScaler is applied to each of the columns. This way, each value is subtracted with the mean of its column and divided by its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the numerical columns \n",
    "numericalColumns = [col for col in df_pd.select_dtypes(include=[float,int]).columns if col not in ['SALEPRICE']]\n",
    "print(\"Numerical columns : \" )\n",
    "print(numericalColumns)\n",
    "\n",
    "scaler_numerical = StandardScaler()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[('scale',scaler_numerical)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously, each of the techniques are grouped by the columns they needed to be applied on and are queued using the ColumnTransformer. Ideally, this is run in the pipeline just before the model is trained. However, to understand what the data will look like, we have transformed the data into a temporary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns)],\n",
    "                                                      remainder=\"passthrough\")\n",
    "preprocessorForAllColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns),('num',numerical_transformer,numericalColumns)],\n",
    "                                              remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "#. The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like\n",
    "df_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_pd_temp)\n",
    "\n",
    "df_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_pd)\n",
    "print(\"Data after transforming :\")\n",
    "print(df_pd_temp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the popular preprocessing steps that are applied on the data sets. You can get more information in Data preprocessing in detail.\n",
    "\n",
    "For more examples, take a look at the [Data preprocessing in detail](https://developer.ibm.com/articles/data-preprocessing-in-detail/) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data frame for splitting data into train and test datasets\n",
    "\n",
    "features = []\n",
    "features = df_pd.drop(['SALEPRICE'], axis=1)\n",
    "\n",
    "label = pd.DataFrame(df_pd, columns = ['SALEPRICE']) \n",
    "#label_encoder = LabelEncoder()\n",
    "label = df_pd['SALEPRICE']\n",
    "\n",
    "#label = label_encoder.fit_transform(label)\n",
    "print(\" value of label : \" + str(label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_slr\"></a>\n",
    "## 5. Build Simple Linear Regression model\n",
    "[Top](#top)\n",
    "\n",
    "This is the most basic form of linear regression in which the variable to be predicted is dependent on only one other variable. This is calculated by using the formula that is generally used in calculating the slope of a line.\n",
    "\n",
    "y = w0 + w1*x1\n",
    "\n",
    "In the above equation, y refers to the target variable and x1 refers to the independent variable. w1 refers to the coefficient that expresses the relationship between y and x1. It is also known as the slope. w0 is the constant coefficient a.k.a the intercept. It refers to the constant offset that y will always be with respect to the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since simple linear regression assumes that output depends on only one variable, we are assuming that it depends on the YEARBUILT. Of course, this will not be the most useful model as it is ignoring all but one column. But it is a good starting point and helps us get familiar with the syntax.\n",
    "\n",
    "**Important Note:**\n",
    "Data is split up into training and test sets. This is a common practice where we split the data into two sets before training our model: train and test. Training Data is what the ML algorithm looks at to learn the patterns while the Test portion is never shown to the model during the training. Once the training is complete, we show the previously unseen data set to our model and compare its predictions with the actual values that we have.\n",
    "\n",
    "To see why this is important, imagine if we used all the data for training and created a model that effectively memorized all the input/output pairs. Now, if we compare our models predictions to the actual labels that we have they will match 100% (since the model memorized them.) However, if you show any new data to the model it will perform poorly because it hasn't really uncovered any real pattern!\n",
    "\n",
    "This is why we keep a portion of the data out during the training phase so that we can evaluate how well our model generalizes after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = features['YEARBUILT'].values.reshape(-1,1)\n",
    "X_train_slr, X_test_slr, y_train_slr, y_test_slr = train_test_split(X,label , random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train_slr.shape)+ \n",
    "      \" Output label\" + str(y_train_slr.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test_slr.shape)+ \n",
    "      \" Output label\" + str(y_test_slr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Our model's name\n",
    "model_name = 'Simple Linear Regression'\n",
    "\n",
    "# Assign the LinearRegression class (imported above)\n",
    "# to a variable so we can use it more simply\n",
    "slRegressor = LinearRegression()\n",
    "\n",
    "# Train the model by calling .fit() method on it\n",
    "slRegressor.fit(X_train_slr,y_train_slr)\n",
    "\n",
    "# Perform prediction on the Test portion of the data\n",
    "y_pred_slr= slRegressor.predict(X_test_slr)\n",
    "\n",
    "print(\"Predictions vs Real labels\")\n",
    "print(pd.DataFrame({\n",
    "                    'predictions' : y_pred_slr, \n",
    "                    'actual values' : y_test_slr.values\n",
    "                    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a linear regression, we can easily print the intercept and coefficient of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: \\n',slRegressor.intercept_)\n",
    "print('Coefficients: \\n', slRegressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_vs_three(X_test_slr[:,0],   # Isolating the first column\n",
    "             y_test_slr,        # Actual values of the sale price\n",
    "             y_pred_slr,        # Predicted values of the sale price\n",
    "             None, True)\n",
    "\n",
    "# This will create a single graph only, Year Built vs price. This is because we are studying a linear regression so \n",
    "# we are looking at only one input variable. Therefore, a 3d plot doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test_slr,y_test_slr,y_pred_slr,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how well our model is performing by looking at the R^2 and MSE values. Note that we don't expect this model to be doing too well because it's using only a single variable from the whole input, and the following metrics also confirm that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember slr stands for Simple Linear Regression\n",
    "slrMetrics = model_metrics(y_test_slr,y_pred_slr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_mlr\"></a>\n",
    "## 6. Build Multiple Linear Regression model\n",
    "[Top](#top)\n",
    "\n",
    "Multiple linear regression is an extension to the simple linear regression. In this setup, the target value is dependent on more than one variable. The number of variables depends on the use case at hand. Usually a subject matter expert is involved in identifying the fields that will contribute towards better predicting the output feature.\n",
    "\n",
    "y = w0 + w1*x1 + w2*x2 + .... + wn*xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features,label , random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n",
    "      \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n",
    "      \" Output label\" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_name = 'Multiple Linear Regression'\n",
    "\n",
    "mlRegressor = LinearRegression()\n",
    "\n",
    "mlr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', mlRegressor)])\n",
    "\n",
    "mlr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_mlr= mlr_model.predict(X_test)\n",
    "\n",
    "print(mlRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have many coefficients and intercepts. This is because we are not solving a multiple linear regression problem instead of a simple linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: \\n',mlRegressor.intercept_)\n",
    "print('Coefficients: \\n', mlRegressor.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_vs_three(X_test['YEARBUILT'],y_test,y_pred_mlr,X_test['LOTAREA'], False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test['YEARBUILT'],y_test,y_pred_mlr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrMetrics = model_metrics(y_test,y_pred_mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_plr\"></a>\n",
    "## 7. Build Polynomial Linear Regression model\n",
    "[Top](#top)\n",
    "\n",
    "The prediction line generated by simple/linear regression is usually a straight line and captures a first order relationship between each colum and the output (label). In cases when a simple or multiple linear regression does not fit the data point accurately, we use the polynomial linear regression. The following formula is used in the back-end to generate polynomial linear regression.\n",
    "\n",
    "y = w0 + w1*x1 + w2*x21 + .... + wn*xnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming that output depends on the YEARBUILT and LOTAREA. Data is split up into training and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this section as a practice and try to fill in the blocks yourself. Where you need help, uncomment the `%load ...` line by removing the leading `# ` and run the cell. That will load the answer for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.iloc[:, [0,4]].values\n",
    "\n",
    "# Exercise \n",
    "# Split the data to train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "X = features.iloc[:, [0,4]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,label, random_state=0)\n",
    "\n",
    "print(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n",
    "      \" Output label\" + str(y_train.shape))\n",
    "print(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n",
    "      \" Output label\" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "model_name = 'Polynomial Linear Regression'\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=3)\n",
    "plRegressor = LinearRegression()\n",
    "\n",
    "plr_model = Pipeline(steps=[('polyFeature',polynomial_features ),('regressor', plRegressor)])\n",
    "\n",
    "# Exercise\n",
    "# train the plr model\n",
    "# make predictions for X_test using your model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "plr_model.fit(X_train,y_train)\n",
    "y_pred_plr= plr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# print the intercepts and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "print('Intercept: \\n',plRegressor.intercept_)\n",
    "print('Coefficients: \\n', plRegressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again let's take a look at how out predictions compare with the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_vs_three(X_test[:,1],y_test,y_pred_plr,X_test[:,0], False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_compare(X_test[:,1],y_test,y_pred_plr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# Finally, use the model_metrics() method to compute MSE and R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "plrMetrics = model_metrics(y_test,y_pred_plr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bbb4d6513b0b5efbd36c1ef5422e93f6429f326203dad9edcb1bcb44bfce58f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('shims': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
